{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "id": "MXgC5fcr7xkH",
        "outputId": "a5176e8b-8220-4269-cbff-822bd3f86c98"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-d5df0069828e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    130\u001b[0m   )\n\u001b[1;32m    131\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dwQFdwSl8KKK",
        "outputId": "f9fb9c9a-938c-4162-de74-529925d3745d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import lib\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from scipy import spatial\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import RegexpTokenizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "from gensim.models import KeyedVectors\n",
        "import pickle\n",
        "from collections import defaultdict"
      ],
      "metadata": {
        "id": "4Ccmxn149c_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Load Data"
      ],
      "metadata": {
        "id": "67ln3FR-9euk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the training data from a file\n",
        "train = pd.read_csv('/content/drive/MyDrive/chatbot_data/dreaddit-train.csv')\n",
        "test = pd.read_csv('/content/drive/MyDrive/chatbot_data/dreaddit-test.csv')\n",
        "# Access the 'subreddit' column\n",
        "subred = train['subreddit']\n",
        "# returns the number of elements in the 'text' column of the DataFrame 'train_df'\n",
        "print(len(train['text']))\n",
        "# Print an array or list of unique values from the 'subred' column\n",
        "subred.unique()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qU31x7qt9deX",
        "outputId": "7ed892d8-a4f6-4744-f8cb-338f7aedfddc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2838\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['ptsd', 'assistance', 'relationships', 'survivorsofabuse',\n",
              "       'domesticviolence', 'anxiety', 'homeless', 'stress',\n",
              "       'almosthomeless', 'food_pantry'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(test['text'][10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H7L9dOCOQ_Uw",
        "outputId": "31ce2915-8ef9-4895-d019-1e93ef2d83b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I've always hated nail files. Somehow that's a part of this. God. I'm confused by it all. It's a feeling to recall it that I've carried my whole life but never understood like a cloud.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preprocess Data"
      ],
      "metadata": {
        "id": "C_eIVAUl9xIS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Tokenization, Removal of punctuations"
      ],
      "metadata": {
        "id": "VvTyOXMF97P3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Using Regexp from the nltk.tokenizer\n",
        "tokenizer = RegexpTokenizer(r'[a-zA-Z]{2,}') #remove number and words has only one character\n",
        "#The tokenization process is applied to the 'text' column of both the 'train' and 'test' dataframes using the apply() function\n",
        "train['processed_text'] = train['text'].apply(lambda x: tokenizer.tokenize(x.lower()))\n",
        "test['processed_text'] = test['text'].apply(lambda x: tokenizer.tokenize(x.lower()))\n",
        "#'processed_text' is a new column that will be created in the train DataFrame\n",
        "#lower() makes it lowercase\n",
        "\n",
        "#Processed_text is a new column that is being added to the train DataFrame."
      ],
      "metadata": {
        "id": "_Ib0aSIH9vNG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Filtering stopwords"
      ],
      "metadata": {
        "id": "8rzx30-b98w2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def filtering_stopwords(data): #function that takes as input 'data'\n",
        "  words = [w for w in data if not w in stopwords.words('english')]\n",
        "  return words\n",
        "\n",
        "# w is a variable that represents an individual word or token from the data list\n",
        "#The 'english' parameter is used to specify the language for which we want to retrieve the stopwords."
      ],
      "metadata": {
        "id": "YzYu9DmE92tS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train['processed_text'] = train['processed_text'].apply(lambda x: filtering_stopwords(x))\n",
        "test['processed_text'] = test['processed_text'].apply(lambda x: filtering_stopwords(x))\n",
        "\n",
        "#x represents each element (text entry) in the text column of the train DataFrame\n",
        "#or x represents each individual element or value in the 'processed_text' column of the DataFrame.\n",
        "#It is a placeholder variable that represents the input to the lambda function,\n",
        "# which in this case is each element of the 'processed_text' column"
      ],
      "metadata": {
        "id": "Dr5FoO1J-JwP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train['processed_text']\n",
        "# is a column in the train DataFrame.\n",
        "#It represents the processed text data after applying various text preprocessing steps\n",
        "#such as tokenization, lowercase conversion, stop word removal, and stemming.\n",
        "#Each element in the processed_text column corresponds to a list of processed words or\n",
        "#tokens for the corresponding text entry in the original text column of the train DataFrame."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2myBQuHA-Mau",
        "outputId": "ec4f29b8-a62d-4aeb-9148-ccdb871d337a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       [said, felt, way, suggeted, go, rest, trigger,...\n",
              "1       [hey, assistance, sure, right, place, post, go...\n",
              "2       [mom, hit, newspaper, shocked, would, knows, l...\n",
              "3       [met, new, boyfriend, amazing, kind, sweet, go...\n",
              "4       [october, domestic, violence, awareness, month...\n",
              "                              ...                        \n",
              "2833    [week, ago, precious, ignored, jan, happy, yea...\n",
              "2834    [ability, cope, anymore, trying, lot, things, ...\n",
              "2835    [case, first, time, reading, post, looking, pe...\n",
              "2836    [find, normal, good, relationship, main, probl...\n",
              "2837    [talking, mom, morning, said, sister, trauma, ...\n",
              "Name: processed_text, Length: 2838, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "def word_lemmatizer(data):\n",
        "  lemma_text = [lemmatizer.lemmatize(i) for i in data]\n",
        "  return lemma_text"
      ],
      "metadata": {
        "id": "4JmLIZXE-Onx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train['processed_text'] = train['processed_text'].apply(lambda x: word_lemmatizer(x))\n",
        "test['processed_text'] = test['processed_text'].apply(lambda x: word_lemmatizer(x))"
      ],
      "metadata": {
        "id": "F01iTrT_-RXb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train['processed_text']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mSyAXFcq-RlF",
        "outputId": "e48a1026-dc39-4efd-c988-67d81b7b0d9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       [said, felt, way, suggeted, go, rest, trigger,...\n",
              "1       [hey, assistance, sure, right, place, post, go...\n",
              "2       [mom, hit, newspaper, shocked, would, know, li...\n",
              "3       [met, new, boyfriend, amazing, kind, sweet, go...\n",
              "4       [october, domestic, violence, awareness, month...\n",
              "                              ...                        \n",
              "2833    [week, ago, precious, ignored, jan, happy, yea...\n",
              "2834    [ability, cope, anymore, trying, lot, thing, t...\n",
              "2835    [case, first, time, reading, post, looking, pe...\n",
              "2836    [find, normal, good, relationship, main, probl...\n",
              "2837    [talking, mom, morning, said, sister, trauma, ...\n",
              "Name: processed_text, Length: 2838, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Word Embedding"
      ],
      "metadata": {
        "id": "-crMgaGy-YtD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre-trained GloVe embeddings\n",
        "word2vec_output_file = '/content/drive/MyDrive/chatbot_data/glove.6B.100d.txt'\n",
        "from gensim.models import KeyedVectors\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "\n",
        "# glove_file = 'glove.6B.100d.txt'  # path to the GloVe embeddings file\n",
        "word2vec_file = '/content/drive/MyDrive/chatbot_data/glove.6B.100d.word2vec'  # output path for the converted Word2Vec file\n",
        "\n",
        "# Convert GloVe file to Word2Vec format\n",
        "glove2word2vec(word2vec_output_file, word2vec_file)\n",
        "\n",
        "# Load the Word2Vec model\n",
        "# model = KeyedVectors.load_word2vec_format(word2vec_file, binary=False)\n",
        "\n",
        "# word2vec_output_file = 'glove.6B.100d.txt'+'.word2vec'\n",
        "# Load pre-trained GloVe embeddings\n",
        "smodel = KeyedVectors.load_word2vec_format(word2vec_file, binary=False)\n",
        "# the glove2word2vec() function to convert the GloVe word embedding file located at '/content/drive/MyDrive/glove.6B.100d.txt'\n",
        "# into the Word2Vec format and save it to the word2vec_output_file path, which is set to '/content/drive/MyDrive/glove.6B.100d.txt'.\n",
        "# The converted Word2Vec file will have the same content as the original GloVe file, but it will be in the Word2Vec format,\n",
        "# which can be loaded and used with the KeyedVectors.load_word2vec_format() method.\n",
        "\n",
        "#KeyedVectors.load_word2vec_format() function in Gensim allows you to represent words as vectors.\n",
        "#Word embeddings capture the semantic relationships between words by mapping them to dense vector representations in a high-dimensional space."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ukdkJRtt-acK",
        "outputId": "81dddd3d-0b80-49a3-ef3a-bc77364f4b9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-21-301d8274eaca>:10: DeprecationWarning: Call to deprecated `glove2word2vec` (KeyedVectors.load_word2vec_format(.., binary=False, no_header=True) loads GLoVE text vectors.).\n",
            "  glove2word2vec(word2vec_output_file, word2vec_file)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "keys = ['abuse', 'anxiety', 'financial', 'ptsd', 'social', 'stress', 'love', 'friendship']\n",
        "for i in keys:\n",
        "  print('Vector of:', i ,smodel.get_vector(i))\n",
        "  result = smodel.most_similar(positive=i, topn=10)\n",
        "  print('\\n10 most similar words to: ',i,result)"
      ],
      "metadata": {
        "id": "-iGnSl8M-eiU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "808b815f-e179-4822-d403-8bafeb033219"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector of: abuse [ 0.6747     0.31766    0.19427   -0.23009   -0.7347     1.4631\n",
            " -0.6046    -0.098757   0.054897   0.23114   -0.074416   0.088503\n",
            " -0.26545    1.1766     0.32394    0.31836    0.073733  -0.019944\n",
            " -1.5462     0.44651   -0.26256   -0.26111   -0.32883    0.60584\n",
            " -0.66182    0.028531   0.61704   -0.34273   -0.20538    0.77077\n",
            "  1.0128    -0.14495    0.0048883  0.63247   -0.26677   -0.19595\n",
            " -0.44454   -0.3079    -0.21728    0.38524   -0.83284    0.18319\n",
            " -0.083784  -0.66435   -0.13656    0.26515    0.34094   -0.27263\n",
            "  0.0086811 -0.76744    0.83997   -0.44614   -0.71315    1.2288\n",
            "  0.16899   -1.0177    -0.16239   -0.90029    1.3657     0.17637\n",
            "  0.051633  -0.068679   0.16901   -0.95653    0.55789   -0.12813\n",
            "  0.95465   -1.1387     0.61918    0.20519    0.18888    0.029914\n",
            " -0.68286    0.28326    0.36491    0.071344  -0.23957    0.32145\n",
            " -1.1288     0.39188    0.83201    0.45375    0.59285    0.50722\n",
            " -2.0063    -0.72023   -0.12334    0.24201   -0.24597   -0.65065\n",
            " -0.39714   -0.44392   -0.27164    0.042077   1.3559    -0.019485\n",
            " -0.35694   -0.11555    0.53027   -0.3727   ]\n",
            "\n",
            "10 most similar words to:  abuse [('sexual', 0.7727022767066956), ('sex', 0.7388114929199219), ('abuses', 0.7365544438362122), ('allegations', 0.7294367551803589), ('rape', 0.7241652607917786), ('harassment', 0.7186876535415649), ('misconduct', 0.704033374786377), ('torture', 0.6989932060241699), ('corruption', 0.6851984262466431), ('child', 0.6768219470977783)]\n",
            "Vector of: anxiety [ 0.0066306  0.1828     0.67915    0.10635   -0.98088    0.51839\n",
            " -0.80593   -0.88419    0.14702    0.014602  -0.86182    0.04276\n",
            " -0.51645   -0.37693    0.82099    0.011972  -0.76182    0.042818\n",
            " -0.24961   -0.579     -0.11956   -0.1257    -0.67124    0.02598\n",
            " -0.54218   -0.033592   0.68791   -0.28887    0.086576  -0.12568\n",
            " -0.094109   0.10699   -0.41816    0.21058   -0.67646    0.23273\n",
            "  0.93634    0.36526    0.21686    0.14149   -0.32114   -0.20877\n",
            " -0.25064   -0.69067    0.3278    -0.1785     0.60967    0.33788\n",
            "  0.089491  -0.82491   -0.13942   -0.38754   -0.31975    0.66951\n",
            "  0.55314   -1.4863     0.71065   -0.17214   -0.0026193  0.21807\n",
            "  0.73602    1.1448     0.66654   -0.3916    -0.10016    0.7296\n",
            "  0.69991   -1.138      0.80996   -0.29561   -0.29745   -0.20458\n",
            " -0.71795    0.23659    0.096237   1.1553     0.12702   -0.22309\n",
            " -0.96484    0.013153   0.16529   -0.044983  -0.42449    0.24305\n",
            " -1.3525    -0.15527   -0.23521    0.028487  -0.87109   -0.50601\n",
            " -0.74841    0.3311    -0.075076   0.23237    0.34843    0.778\n",
            "  1.1292    -0.66228    0.20343    0.18071  ]\n",
            "\n",
            "10 most similar words to:  anxiety [('anger', 0.7617689967155457), ('nervousness', 0.733610212802887), ('frustration', 0.7292328476905823), ('discomfort', 0.7233244776725769), ('paranoia', 0.7181074619293213), ('confusion', 0.7138597369194031), ('anxieties', 0.711757242679596), ('stress', 0.709513783454895), ('despair', 0.7053972482681274), ('feelings', 0.6982271075248718)]\n",
            "Vector of: financial [ 0.33134  -0.04661   0.10657  -0.26801  -0.27547  -0.73264  -0.9775\n",
            " -0.64882   0.062462 -0.20076   0.65747   0.24898   0.10554   0.11204\n",
            " -0.53015  -0.18722   0.27637  -0.33475  -0.23617  -0.20215  -0.41184\n",
            "  0.028952 -0.298     0.16072  -0.66309  -0.34644   0.10096  -0.2311\n",
            " -0.97776   0.28702   0.54741   0.28516  -0.92817  -0.32457  -0.9536\n",
            " -0.31486   0.39549   0.14776  -0.456     0.15904  -0.46332  -0.91914\n",
            " -0.1546    0.13042  -0.1438    0.39351   0.32141   0.085901 -0.2182\n",
            " -0.87695  -0.5807    0.071227  0.19958   0.54646   0.35967  -2.7143\n",
            "  0.44322  -0.56998   1.8921    0.81757  -0.26195  -0.19525  -1.3027\n",
            " -0.099452  0.3325   -0.79673  -0.35013   0.23426   1.3519   -0.16837\n",
            " -0.72781   0.036372 -0.98335  -0.42376   0.1248   -0.39884  -0.38062\n",
            "  0.34085  -1.0434    0.040638  1.5705    0.24799   0.12334   0.043277\n",
            " -1.3124   -0.37536  -0.49102  -0.19736  -0.20121  -0.88124   0.23803\n",
            "  0.47244   0.27429  -0.40834   0.26192   0.99651   0.98947  -0.6365\n",
            "  0.67295   0.62477 ]\n",
            "\n",
            "10 most similar words to:  financial [('banking', 0.7891805171966553), ('corporate', 0.7738927602767944), ('economic', 0.7714813947677612), ('credit', 0.7692809700965881), ('investment', 0.7671132683753967), ('business', 0.7482263445854187), ('global', 0.7365725040435791), ('management', 0.734153151512146), ('fund', 0.7081953883171082), ('banks', 0.7058879137039185)]\n",
            "Vector of: ptsd [-0.323     0.93968  -0.14216   0.07845  -1.5524   -0.31777  -0.13789\n",
            " -0.51181   0.511    -0.10846  -0.30864  -0.41167  -0.097314  0.80191\n",
            "  1.5308    0.041924  0.23779  -0.97669  -0.099634 -0.17963  -0.037075\n",
            " -0.046191 -1.1301    0.8084   -0.61865  -0.45362   0.6355   -0.35339\n",
            "  0.31447  -0.24616   0.60796   0.89891  -0.53141   0.31569   0.25266\n",
            " -0.14764   0.11194   0.59363  -0.64152   0.046084  0.039019  1.1668\n",
            " -0.083849 -0.21478  -0.12963   0.16848   0.74785  -0.01945  -0.47476\n",
            " -0.69543   0.46255  -0.65407  -0.42366  -0.027828  0.44531   0.483\n",
            "  1.235    -0.38235  -0.26627   0.85118   0.2935    1.8751    1.1202\n",
            " -0.32175  -0.010322  0.80081   0.10104  -0.98285   0.015198  0.73697\n",
            "  0.13984   0.60534  -0.76027   0.16573   0.62429   0.14766  -0.51643\n",
            "  0.76707  -0.027361 -0.29336  -0.062604  0.061716  0.075125 -0.096013\n",
            " -0.54888   0.30848   0.56884   0.71552  -0.64725  -0.11532  -0.32666\n",
            "  0.13587   0.6246   -0.18767   0.85372   0.20314   1.0594   -0.043378\n",
            "  0.063829 -0.42204 ]\n",
            "\n",
            "10 most similar words to:  ptsd [('traumatic', 0.708774983882904), ('post-traumatic', 0.6651941537857056), ('disorder', 0.6327651739120483), ('posttraumatic', 0.6266862750053406), ('fibromyalgia', 0.6164780855178833), ('psychosis', 0.6153751611709595), ('trauma', 0.6145432591438293), ('schizophrenia', 0.6127398610115051), ('adhd', 0.6126563549041748), ('disorders', 0.5918419361114502)]\n",
            "Vector of: social [-0.24861   0.65492  -0.34935   0.22602   0.32131   0.33169  -0.69937\n",
            " -0.79396   0.36112   0.45019  -0.5848   -0.26079  -0.20359   0.11051\n",
            "  0.56347  -0.28351   0.69255   0.061113 -0.40681   0.17453  -0.26842\n",
            " -0.044954  0.22822   0.14171  -0.42381  -1.0615   -0.099736 -0.61057\n",
            " -0.16646  -0.10446  -0.21737   0.94042  -0.30025  -0.067251 -0.40675\n",
            " -0.1696    0.19566   0.61129  -0.75535   0.34861  -0.81396  -0.44849\n",
            " -0.13668   0.11478  -0.6047   -0.33903   0.039049  0.45745  -0.60466\n",
            "  0.22668   0.26345  -0.2814   -0.54628   0.87143   0.46428  -2.198\n",
            "  1.1758   -0.039133  1.8234    0.88758   0.12617  -0.23075  -0.91688\n",
            " -0.59195   1.3559    0.40534  -0.047916 -0.63889   1.8571   -0.27157\n",
            "  0.061506 -0.016188 -0.1973   -0.45165  -0.38345  -0.067396 -0.30852\n",
            " -0.15313  -0.57111  -0.048921 -0.20939   0.31559   0.58533  -0.10424\n",
            " -1.8253   -0.030741 -0.69598   0.2492   -0.53853  -1.1237    0.6469\n",
            " -0.55666   0.27738   0.57313  -0.038488  0.056588  0.24121  -0.96586\n",
            "  0.79163   0.12448 ]\n",
            "\n",
            "10 most similar words to:  social [('education', 0.7497826814651489), ('political', 0.7463673949241638), ('cultural', 0.7148114442825317), ('welfare', 0.7065821886062622), ('educational', 0.6951513886451721), ('reform', 0.6909914612770081), ('health', 0.6869199872016907), ('public', 0.6821627616882324), ('environment', 0.6808214783668518), ('environmental', 0.6754921078681946)]\n",
            "Vector of: stress [-0.79252    0.1112    -0.15714    0.33814   -1.5105    -0.2456\n",
            " -1.0716    -0.35966   -0.62459    0.21511   -0.35896   -0.38863\n",
            "  0.15035    0.47511    0.34397    0.092767  -0.76359   -0.32845\n",
            " -0.18058   -0.69679    0.025618  -0.22498   -1.0008     0.44016\n",
            " -0.42199    0.2957    -0.15925   -0.46405    0.37978   -0.035413\n",
            " -0.2422     0.15262   -0.27078    0.062017  -0.17211   -0.24836\n",
            "  0.4039     0.4822    -0.44304    0.40773   -0.44366    0.088745\n",
            " -0.022201  -0.49345   -0.31535    0.26685    0.041346   0.10019\n",
            " -0.30001   -1.0617     0.19757   -0.77107   -0.23166    0.75719\n",
            "  0.7147    -1.4065     0.53922   -0.85026    0.8112     0.93463\n",
            "  0.38038    0.85097   -0.096784  -0.44867    0.74315    0.81144\n",
            "  0.13238   -0.76748    0.42411   -0.3287    -0.43828    0.1506\n",
            " -0.077817   0.30273    0.63116    0.67563    0.14995   -0.34982\n",
            " -0.17312    0.11598    0.28339   -0.35725   -0.88685   -0.0050747\n",
            " -1.6313     0.40505    0.23601    0.12228   -1.0897    -0.28835\n",
            " -0.65577   -0.2405     0.12842    0.33902    0.81716    0.31335\n",
            "  0.62835   -1.1465     0.6587     0.12363  ]\n",
            "\n",
            "10 most similar words to:  stress [('pain', 0.725872278213501), ('anxiety', 0.7095138430595398), ('fatigue', 0.7049359679222107), ('mental', 0.701725423336029), ('psychological', 0.6904554963111877), ('physical', 0.6795199513435364), ('severe', 0.6727946400642395), ('problems', 0.6613970994949341), ('risk', 0.6541153788566589), ('muscle', 0.645975649356842)]\n",
            "Vector of: love [ 2.5975e-01  5.5833e-01  5.7986e-01 -2.1361e-01  1.3084e-01  9.4385e-01\n",
            " -4.2817e-01 -3.7420e-01 -9.4499e-02 -4.3344e-01 -2.0937e-01  3.4702e-01\n",
            "  8.2516e-02  7.9735e-01  1.6606e-01 -2.6878e-01  5.8830e-01  6.7397e-01\n",
            " -4.9965e-01  1.4764e+00  5.5261e-01  2.5295e-02 -1.6068e-01 -1.3878e-01\n",
            "  4.8686e-01  1.1420e+00  5.6195e-02 -7.3306e-01  8.6932e-01 -3.5892e-01\n",
            " -5.1877e-01  9.0402e-01  4.9249e-01 -1.4915e-01  4.8493e-02  2.6096e-01\n",
            "  1.1352e-01  4.1275e-01  5.3803e-01 -4.4950e-01  8.5733e-02  9.1184e-02\n",
            "  5.0177e-03 -3.4645e-01 -1.1058e-01 -2.2235e-01 -6.5290e-01 -5.1838e-02\n",
            "  5.3791e-01 -8.1040e-01 -1.8253e-01  2.4194e-01  5.4855e-01  8.7731e-01\n",
            "  2.2165e-01 -2.7124e+00  4.9405e-01  4.4703e-01  5.5882e-01  2.6076e-01\n",
            "  2.3760e-01  1.0668e+00 -5.6971e-01 -6.4960e-01  3.3511e-01  3.4609e-01\n",
            "  1.1033e+00  8.5261e-02  2.4847e-02 -4.5453e-01  7.7012e-02  2.1321e-01\n",
            "  1.0444e-01  6.7157e-02 -3.4261e-01  8.5534e-01  1.3361e-01 -4.3296e-01\n",
            " -5.6726e-01 -2.1348e-01 -3.3277e-01  3.4351e-01  3.2164e-01  4.4527e-01\n",
            " -1.3208e+00 -1.3270e-01 -7.0820e-01 -4.8472e-01 -6.9396e-01 -2.6080e-01\n",
            " -4.7099e-01 -5.7492e-02  9.3587e-02  4.0006e-01 -4.3419e-01 -2.7364e-01\n",
            " -7.7017e-01 -8.4028e-01 -1.5620e-03  6.2223e-01]\n",
            "\n",
            "10 most similar words to:  love [('me', 0.7382813692092896), ('passion', 0.7352136373519897), ('my', 0.7327208518981934), ('life', 0.7287957668304443), ('dream', 0.7267670035362244), ('you', 0.7181724905967712), ('always', 0.7111518979072571), ('wonder', 0.70945805311203), ('i', 0.7084634304046631), ('dreams', 0.7067317962646484)]\n",
            "Vector of: friendship [ 0.51813    0.79066   -0.15113    0.99996    0.27108   -0.39643\n",
            " -0.02862   -0.54229   -0.39664   -0.43319   -1.2005    -0.20581\n",
            "  1.0134     0.42438    0.10026   -0.34857    0.86265   -0.60788\n",
            " -0.9637     0.059907  -0.10289    0.090389  -0.54098    0.0088074\n",
            "  0.62559    0.37331   -0.050293  -0.39242    0.55023   -0.15198\n",
            " -0.57178    1.389      0.56011   -0.37128    0.067883   0.036163\n",
            "  0.99142   -0.078035  -0.45492   -0.0037057  0.004526  -0.094208\n",
            "  0.72276   -0.41788    0.085468  -0.46014    0.053003   0.54909\n",
            "  0.50657   -0.60181   -0.3263    -0.14771    0.057282   0.94446\n",
            "  0.55029   -1.6908     0.09723   -0.040324   0.59984    0.23109\n",
            " -0.53376    0.58561   -0.29401   -0.44921   -0.50741    0.67845\n",
            "  0.19392    0.61172    0.59049   -0.72692    0.96463    0.073383\n",
            "  0.72056   -0.91361    0.39574    0.93179    0.3528    -0.40029\n",
            " -1.1393     0.56441   -0.37816    1.1779     1.2567     0.10104\n",
            " -0.47704    1.0889    -0.55661   -0.0022083 -0.37627   -0.66177\n",
            " -0.40627    0.12851   -0.33155    0.12291   -0.0092895  0.11307\n",
            " -0.43478   -0.68978   -0.55796    0.94777  ]\n",
            "\n",
            "10 most similar words to:  friendship [('relationship', 0.7378768920898438), ('relations', 0.7214967608451843), ('understanding', 0.6997854709625244), ('bilateral', 0.6785918474197388), ('ties', 0.6776943802833557), ('friendships', 0.6729941964149475), ('cooperation', 0.6710678935050964), ('partnership', 0.6621981263160706), ('mutual', 0.653221607208252), ('neighborly', 0.6428439617156982)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#TF-IDF Weighted Average"
      ],
      "metadata": {
        "id": "zl1d4W5E-tWp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TfidfEmbeddingVectorizer(object):\n",
        "\n",
        "    def __init__(self, word_model):\n",
        "        self.word_model = word_model\n",
        "         #word_model represents the pre-trained word embedding model\n",
        "        self.word_idf_weight = None\n",
        "        #This attribute will later store the IDF (Inverse Document Frequency) weights of the words.\n",
        "        self.vector_size = word_model.vector_size\n",
        "        #The vector_size attribute is set to the dimensionality of the word vectors in the word_model\n",
        "\n",
        "    def default_idf(self):\n",
        "        return self.max_idf\n",
        "\n",
        "    def fit(self, docs): # comply with scikit-learn transformer requirement\n",
        "      \"\"\"\n",
        "      Fit in a list of docs, which had been preprocessed and tokenized,\n",
        "      such as word bi-grammed, stop-words removed, lemmatized, part of speech filtered.\n",
        "      Then build up a tfidf model to compute each word's idf as its weight.\n",
        "      Noted that tf weight is already involved when constructing average word vectors, and thus omitted.\n",
        "      :param: pre_processed_docs: list of docs, which are tokenized\n",
        "      :return: self\n",
        "      \"\"\"\n",
        "\n",
        "      text_docs = []\n",
        "      for doc in docs:\n",
        "          text_docs.append(\" \".join(doc))\n",
        "\n",
        "      tfidf = TfidfVectorizer()  # default 1-gram\n",
        "      tfidf.fit(text_docs)\n",
        "\n",
        "      self.max_idf = max(tfidf.idf_)\n",
        "      self.word_idf_weight = defaultdict(self.default_idf)\n",
        "\n",
        "      for word, i in tfidf.vocabulary_.items():\n",
        "          self.word_idf_weight[word] = tfidf.idf_[i]\n",
        "\n",
        "      return self\n",
        "\n",
        "\n",
        "    def transform(self, docs):       # comply with scikit-learn transformer requirement\n",
        "        doc_word_vector = self.word_average_list(docs)\n",
        "        return doc_word_vector\n",
        "\n",
        "    def word_average(self, sent):\n",
        "        \"\"\"\n",
        "        Compute average word vector for a single doc/sentence.\n",
        "        :param sent: list of sentence tokens\n",
        "        :return:\n",
        "            mean: float of averaging word vectors\n",
        "        \"\"\"\n",
        "\n",
        "        mean = []\n",
        "        for word in sent:\n",
        "            if word in self.word_model.key_to_index:\n",
        "                vector = self.word_model.get_vector(word)\n",
        "                mean.append(vector * self.word_idf_weight[word])  # idf weighted\n",
        "\n",
        "        if not mean:\n",
        "            return np.zeros(self.vector_size)\n",
        "        else:\n",
        "            mean = np.array(mean).mean(axis=0)\n",
        "            return mean\n",
        "\n",
        "    def word_average_list(self, docs):\n",
        "        \"\"\"\n",
        "        Compute average word vector for multiple docs, where docs had been tokenized.\n",
        "        :param docs: list of sentence in list of separated tokens\n",
        "        :return:\n",
        "            array of average word vector in shape (len(docs),)\n",
        "        \"\"\"\n",
        "        return np.vstack([self.word_average(sent) for sent in docs])\n",
        "\n",
        "\n",
        "    def save(self, filename):\n",
        "        \"\"\"\n",
        "        Save the vectorizer to a file using pickle.\n",
        "        :param filename: Name of the file to save the vectorizer.\n",
        "        \"\"\"\n",
        "        with open(filename, 'wb') as f:\n",
        "            pickle.dump(self, f)\n",
        "\n",
        "\n",
        "    @classmethod\n",
        "    def load(cls, filename):\n",
        "        \"\"\"\n",
        "        Load the vectorizer from a file.\n",
        "        :param filename: Name of the file to load the vectorizer from.\n",
        "        :return: An instance of the TfidfEmbeddingVectorizer class.\n",
        "        \"\"\"\n",
        "        with open(filename, 'rb') as f:\n",
        "            return pickle.load(f)\n",
        "\n",
        "\n",
        "\n",
        "#In summary, this class combines the TF-IDF vectorization technique with pre-trained word embeddings\n",
        "#to transform preprocessed and tokenized documents into average word vector representations that are weighted by IDF.\n",
        "#The resulting vectors can be used as input features for various machine learning models or downstream tasks.\n"
      ],
      "metadata": {
        "id": "PfznH6i1sJdk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_vec_tr = TfidfEmbeddingVectorizer(smodel)\n",
        "tfidf_vec_tr = tfidf_vec_tr.fit(train['processed_text'])  # fit tfidf model first\n",
        "X_train = tfidf_vec_tr.transform(train['processed_text'])\n",
        "y_train = train['label']\n",
        "\n",
        "# tfidf_vec_tr.fit(test['processed_text'])\n",
        "X_test = tfidf_vec_tr.transform(test['processed_text'])\n",
        "y_test = test['label']"
      ],
      "metadata": {
        "id": "W69Wm12G_YVw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the vectorizer\n",
        "tfidf_vec_tr.save('/content/drive/MyDrive/chatbot_data/tfidf_vec_tr.pkl')"
      ],
      "metadata": {
        "id": "fTfF9PDJ_ZZf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the weights\n",
        "# Save the weights to a file\n",
        "with open('/content/drive/MyDrive/chatbot_data/word_idf_weights.pkl', 'wb') as file:\n",
        "    pickle.dump(tfidf_vec_tr.word_idf_weight, file)"
      ],
      "metadata": {
        "id": "JDeud7-SP8mV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tfidf_vec_tr.word_idf_weight)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9irAIN3RPmje",
        "outputId": "07872dea-af83-408e-e8d2-5f22aabfb766"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-0.847684    0.68693095  0.45550036 ...  0.14931832  0.7828862\n",
            "   0.9441152 ]\n",
            " [ 0.13888466  1.6395565   1.0083388  ... -0.29444835  2.1498945\n",
            "   0.60911167]\n",
            " [ 0.14925882  0.7516254   1.1579771  ...  0.20874952  1.1069926\n",
            "   0.11873502]\n",
            " ...\n",
            " [-0.11177807  0.14724047  1.0613122  ... -0.16153254  1.2265683\n",
            "   0.6959015 ]\n",
            " [-0.8360129   1.3445976   1.4285393  ... -1.4185493   1.7372129\n",
            "   0.7974449 ]\n",
            " [-0.02265701  0.6556465   2.171653   ... -0.0295425   1.7845883\n",
            "   0.17743158]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Train and Save the Model"
      ],
      "metadata": {
        "id": "BTOnymbeALf1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clf_dict = {'log reg': LogisticRegression(random_state=42),\n",
        "            'naive bayes': GaussianNB(),\n",
        "            'linear svc': LinearSVC(random_state=42),\n",
        "            'sgd classifier': SGDClassifier(random_state=42),\n",
        "            'ada boost': AdaBoostClassifier(n_estimators=100, random_state=42),\n",
        "            'gradient boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
        "            'CART': DecisionTreeClassifier(random_state=42),\n",
        "            'random forest': RandomForestClassifier(n_estimators=100, random_state=42)}\n",
        "for name, clf in clf_dict.items():\n",
        "    pred = clf.fit(X_train, y_train)\n",
        "    y_pred = pred.predict(X_test)\n",
        "    print('Accuracy of {}:'.format(name), accuracy_score(y_pred, y_test))"
      ],
      "metadata": {
        "id": "ehGDeTCQAJoh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1bfd594-a18d-4e62-f536-b52bef7a7689"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of log reg: 0.6951048951048951\n",
            "Accuracy of naive bayes: 0.6363636363636364\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of linear svc: 0.6853146853146853\n",
            "Accuracy of sgd classifier: 0.5832167832167832\n",
            "Accuracy of ada boost: 0.6811188811188811\n",
            "Accuracy of gradient boosting: 0.7132867132867133\n",
            "Accuracy of CART: 0.5986013986013986\n",
            "Accuracy of random forest: 0.6951048951048951\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "class MajorityVoteClassifier(object):\n",
        "    def __init__(self, classifiers):\n",
        "        self.classifiers = classifiers\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        for clf in self.classifiers:\n",
        "            clf.fit(X, y)\n",
        "\n",
        "    def predict(self, X):\n",
        "        predictions = []\n",
        "        for clf in self.classifiers:\n",
        "            predictions.append(clf.predict(X))\n",
        "\n",
        "        # Perform majority vote\n",
        "        majority_vote = np.apply_along_axis(lambda x: np.argmax(np.bincount(x)), axis=0, arr=predictions)\n",
        "        return majority_vote\n",
        "\n",
        "    def save(self, filename):\n",
        "        with open(filename, 'wb') as f:\n",
        "            pickle.dump(self, f)\n",
        "\n",
        "    @classmethod\n",
        "    def load(cls, filename):\n",
        "        with open(filename, 'rb') as f:\n",
        "            return pickle.load(f)\n"
      ],
      "metadata": {
        "id": "PzVBAgHPAP6L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifiers = [LogisticRegression(random_state=42),\n",
        "               GaussianNB(), LinearSVC(random_state=42), SGDClassifier(random_state=42), AdaBoostClassifier(n_estimators=100, random_state=42),\n",
        "               GradientBoostingClassifier(n_estimators=100, random_state=42), DecisionTreeClassifier(random_state=42), RandomForestClassifier(n_estimators=100, random_state=42)]\n",
        "\n",
        "for name, clf in clf_dict.items(): # Replace with your list of classifiers\n",
        "  mv_classifier = MajorityVoteClassifier(classifiers)\n",
        "  mv_classifier.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "4HDpRlROA48T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "688c3c0b-09e4-40f1-d400-5927c0c80098"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the combined model\n",
        "mv_classifier.save('/content/drive/MyDrive/chatbot_data/majority_vote_model.pkl')"
      ],
      "metadata": {
        "id": "UeAkhNz1BmwG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}